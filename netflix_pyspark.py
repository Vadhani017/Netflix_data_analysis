# -*- coding: utf-8 -*-
"""netflix_pyspark

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uiXbZ08s6cxeel-V0j0_Cj-yo6AP74uN
"""

pip install pyspark

!pip install nltk  # Ensure that NLTK is installed
import nltk
nltk.download('vader_lexicon')

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from pyspark.sql.functions import udf
from pyspark.sql.types import FloatType

# Create a Spark session
spark = SparkSession.builder.appName("NetflixSentimentAnalysis").getOrCreate()

df = spark.read.csv("/content/Netflix_data_cleaned.csv", header=True, inferSchema=True)

# Display the schema of the DataFrame
df.printSchema()

# Show a sample of the data
df.show(5, truncate=False)

# Perform Sentiment Analysis on descriptions using NLTK
sid = SentimentIntensityAnalyzer()

# Define a UDF to apply sentiment analysis to each description
def get_sentiment(text):
    sentiment_score = sid.polarity_scores(text)
    return sentiment_score['compound']

sentiment_udf = udf(get_sentiment, FloatType())

# Apply sentiment analysis and add a new column 'sentiment_score'
df = df.withColumn('sentiment_score', sentiment_udf(col('description')))

# Display the DataFrame with the new 'sentiment_score' column
df.select('title', 'description', 'sentiment_score').show(truncate=False)

# Summary statistics for numerical columns
df.describe(['release_year', 'duration']).show()

# Number of movies and TV shows in the dataset
df.groupBy('type').count().show()

# Top 5 countries with the most content
top_countries = df.groupBy('country').count().orderBy(col('count').desc()).show(5)

# Average duration of movies and TV shows
df.groupBy('type').agg({'duration': 'avg'}).show()

# Number of titles added per year
titles_added_per_year = df.groupBy('release_year').count().orderBy('release_year').show()

# Genres with the highest number of titles
top_genres = df.groupBy('listed_in').count().orderBy(col('count').desc()).show(5, truncate=False)

# Stop the Spark session
spark.stop()